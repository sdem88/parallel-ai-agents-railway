default:
  model: gpt-3.5-turbo-0125
  temperature: 0.7
  fallback: mixtral-8x7b

headline_rewrite:
  model: mixtral-8x7b
  temperature: 0.3
  max_tokens: 120
  fallback: gpt-3.5-turbo-0125

investigative_deep:
  model: gpt-4o-pro
  temperature: 0.2
  max_tokens: 2048
  fallback: claude-3-opus-20240229

code_gen:
  model: gpt-4o
  temperature: 0.1
  max_tokens: 4096
  fallback: claude-3-opus-20240229

research_paper_summary:
  model: claude-3-opus-20240229
  temperature: 0.2
  max_tokens: 2048
  fallback: gpt-4o

data_analysis:
  model: gpt-4o
  temperature: 0.1
  max_tokens: 3072
  fallback: claude-3-sonnet-20240229